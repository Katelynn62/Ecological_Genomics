# Ecological Genomics Notebook

## Author: Katelynn Warner  
### Affiliation:  University of Vermont Natural Resources PhD Student 
### E-mail contact: kwarner@uvm.edu


### Start Date: 2020-01-13
### End Date: 2020-05-08
### Project Descriptions:   





# Table of Contents:   
* [Entry 1: 2020-01-13, Monday](#id-section1)
* [Entry 2: 2020-01-14, Tuesday](#id-section2)
* [Entry 3: 2020-01-15, Wednesday](#id-section3)
* [Entry 4: 2020-01-16, Thursday](#id-section4)
* [Entry 5: 2020-01-17, Friday](#id-section5)
* [Entry 6: 2020-01-20, Monday](#id-section6)
* [Entry 7: 2020-01-21, Tuesday](#id-section7)
* [Entry 8: 2020-01-22, Wednesday](#id-section8)
* [Entry 9: 2020-01-23, Thursday](#id-section9)
* [Entry 10: 2020-01-24, Friday](#id-section10)
* [Entry 11: 2020-01-27, Monday](#id-section11)
* [Entry 12: 2020-01-28, Tuesday](#id-section12)
* [Entry 13: 2020-01-29, Wednesday](#id-section13)
* [Entry 14: 2020-01-30, Thursday](#id-section14)
* [Entry 15: 2020-01-31, Friday](#id-section15)
* [Entry 16: 2020-02-03, Monday](#id-section16)
* [Entry 17: 2020-02-04, Tuesday](#id-section17)
* [Entry 18: 2020-02-05, Wednesday](#id-section18)
* [Entry 19: 2020-02-06, Thursday](#id-section19)
* [Entry 20: 2020-02-07, Friday](#id-section20)
* [Entry 21: 2020-02-10, Monday](#id-section21)
* [Entry 22: 2020-02-11, Tuesday](#id-section22)
* [Entry 23: 2020-02-12, Wednesday](#id-section23)
* [Entry 24: 2020-02-13, Thursday](#id-section24)
* [Entry 25: 2020-02-14, Friday](#id-section25)
* [Entry 26: 2020-02-17, Monday](#id-section26)
* [Entry 27: 2020-02-18, Tuesday](#id-section27)
* [Entry 28: 2020-02-19, Wednesday](#id-section28)
* [Entry 29: 2020-02-20, Thursday](#id-section29)
* [Entry 30: 2020-02-21, Friday](#id-section30)
* [Entry 31: 2020-02-24, Monday](#id-section31)
* [Entry 32: 2020-02-25, Tuesday](#id-section32)
* [Entry 33: 2020-02-26, Wednesday: Transcriptomics Day 1](#id-section33)
* [Entry 34: 2020-02-27, Thursday](#id-section34)
* [Entry 35: 2020-02-28, Friday](#id-section35)
* [Entry 36: 2020-03-02, Monday](#id-section36)
* [Entry 37: 2020-03-03, Tuesday](#id-section37)
* [Entry 38: 2020-03-04, Wednesday: Transcriptomics Day 2](#id-section38)
* [Entry 39: 2020-03-05, Thursday](#id-section39)
* [Entry 40: 2020-03-06, Friday](#id-section40)
* [Entry 41: 2020-03-09, Monday](#id-section41)
* [Entry 42: 2020-03-10, Tuesday](#id-section42)
* [Entry 43: 2020-03-11, Wednesday](#id-section43)
* [Entry 44: 2020-03-12, Thursday](#id-section44)
* [Entry 45: 2020-03-13, Friday](#id-section45)
* [Entry 46: 2020-03-16, Monday](#id-section46)
* [Entry 47: 2020-03-17, Tuesday](#id-section47)
* [Entry 48: 2020-03-18, Wednesday](#id-section48)
* [Entry 49: 2020-03-19, Thursday](#id-section49)
* [Entry 50: 2020-03-20, Friday](#id-section50)
* [Entry 51: 2020-03-23, Monday](#id-section51)
* [Entry 52: 2020-03-24, Tuesday](#id-section52)
* [Entry 53: 2020-03-25, Wednesday](#id-section53)
* [Entry 54: 2020-03-26, Thursday](#id-section54)
* [Entry 55: 2020-03-27, Friday](#id-section55)
* [Entry 56: 2020-03-30, Monday](#id-section56)
* [Entry 57: 2020-03-31, Tuesday](#id-section57)
* [Entry 58: 2020-04-01, Wednesday](#id-section58)
* [Entry 59: 2020-04-02, Thursday](#id-section59)
* [Entry 60: 2020-04-03, Friday](#id-section60)
* [Entry 61: 2020-04-06, Monday](#id-section61)
* [Entry 62: 2020-04-07, Tuesday](#id-section62)
* [Entry 63: 2020-04-08, Wednesday](#id-section63)
* [Entry 64: 2020-04-09, Thursday](#id-section64)
* [Entry 65: 2020-04-10, Friday](#id-section65)
* [Entry 66: 2020-04-13, Monday](#id-section66)
* [Entry 67: 2020-04-14, Tuesday](#id-section67)
* [Entry 68: 2020-04-15, Wednesday](#id-section68)
* [Entry 69: 2020-04-16, Thursday](#id-section69)
* [Entry 70: 2020-04-17, Friday](#id-section70)
* [Entry 71: 2020-04-20, Monday](#id-section71)
* [Entry 72: 2020-04-21, Tuesday](#id-section72)
* [Entry 73: 2020-04-22, Wednesday](#id-section73)
* [Entry 74: 2020-04-23, Thursday](#id-section74)
* [Entry 75: 2020-04-24, Friday](#id-section75)
* [Entry 76: 2020-04-27, Monday](#id-section76)
* [Entry 77: 2020-04-28, Tuesday](#id-section77)
* [Entry 78: 2020-04-29, Wednesday](#id-section78)
* [Entry 79: 2020-04-30, Thursday](#id-section79)
* [Entry 80: 2020-05-01, Friday](#id-section80)
* [Entry 81: 2020-05-04, Monday](#id-section81)
* [Entry 82: 2020-05-05, Tuesday](#id-section82)
* [Entry 83: 2020-05-06, Wednesday](#id-section83)
* [Entry 84: 2020-05-07, Thursday](#id-section84)
* [Entry 85: 2020-05-08, Friday](#id-section85)

------
<div id='id-section1'/>   


### Entry 1: 2020-01-13, Monday.   



------
<div id='id-section2'/>   

### Entry 2: 2020-01-14, Tuesday.   



------
<div id='id-section3'/> 

### Entry 3: 2020-01-15, Wednesday.   



------
<div id='id-section4'/>   
### Entry 4: 2020-01-16, Thursday.   



------
<div id='id-section5'/>   
### Entry 5: 2020-01-17, Friday.   



------
<div id='id-section6'/>   
### Entry 6: 2020-01-20, Monday.   



------
<div id='id-section7'/>   
### Entry 7: 2020-01-21, Tuesday.   



------
<div id='id-section8'/>   
### Entry 8: 2020-01-22, Wednesday.   



------
<div id='id-section9'/>   
### Entry 9: 2020-01-23, Thursday.   



------
<div id='id-section10'/>   
### Entry 10: 2020-01-24, Friday.   



------
<div id='id-section11'/>   


### Entry 11: 2020-01-27, Monday.   



------
<div id='id-section12'/>   


### Entry 12: 2020-01-28, Tuesday.   



------
<div id='id-section13'/>   


### Entry 13: 2020-01-29, Wednesday.   

* sample size = 110 mother trees from 23 populations
* 80,000 120bp probes -> uniquely hybridized to a specific area
* paired ends: 150 on each side..., so if our strand is 400 bp, we are only missing 100 bp readings.
* single end: Can get 150 bp on one end of a 5'-> 3' DNA 
Pipeline: files will end in .fastq.gz - zip file. 
1. first thing we will do is visualize these (Program FastQC)
2. Clean raw data (Program: Tricommomatic)
* Visualize post trimming step to make sure it did what you wanted it to do (FastQC)
3. Map/Align those reads to the reference genome (Program bwa, input: *.fastq*, output: *.sam*(human readable file)
4. Post processing (samtools, sambamba)
5. Remove PCR duplicates
6. Calculate stats

Coding: 
* Because files are in *.fastq.gz* and are zipped, we cannot use standard bash tools that we learned in last class. 
1. R1 = forward read
2. R2 = reverse read
* zcat peaks into the gzipped file without actually unzipping it. 
1. *be sure to name your heading number otherwise your computer will spit out the contents without stopping* EX: zcat ___ | head -n 4
1. Within this, N refers to a base that did not get a good read.
3. Phred scores - what is the probability that the machine read the base *incorrectly*?
1. p=10^-38/10
2. p=10^-3.8
3. p=0.0038

Running the FastQC Program
##### I am working with the: _BRB_

Bash scripting
* start with 
1. #!/bin/bash - tells your script that you are working in bash
2. for file in /data/project_data/RS_ExomeSeq/fastq/edge_fastq/BRB*fastq.gz
3. To exit edit mode in script, click escape
4. *.sh* is a bash script
5. can rename file by old file name to move to new file name
6. Because our file is not rwxr--r--...(*rwr--r--r*)
1. chmod is a command to change permissions: u+x adds permissions to all users
7. Sync repo with github and then with your local machine - pull github repo...double click html to open the files. Can't open in bash.

#### Trimmomatic
1. Example script is located in cd /data/scripts

We want paired reads here...unpaired reads are lower quality data. 
* To do this, we need to create an R2 name based on the R1 name becuase the are identical...
1. BRB_01_R1_fastqc.gz
2. BRB_01_R2_fastqc.gz
* So we are saying that R1 variable is equal to the R2 variable... *got confused here*
* threads is a CPU in your computer. 
* *phred33* is a phred score..
* Reads are going to be put into the common space..Unpaired and paired reads to separate them. It will name each of the files based on the named variable...BRB_05_R1.cl.pd.fq ,etc.

cleaned paired reads are in *cd data/project_data/RS_ExomeSeq/fastq?edge_fastq/pairedcleanreads/*
* You should have probably 8 files in there. 





------
<div id='id-section14'/>   


### Entry 14: 2020-01-30, Thursday.   



------
<div id='id-section15'/>   


### Entry 15: 2020-01-31, Friday.   



------
<div id='id-section16'/>   


### Entry 16: 2020-02-03, Monday.   



------
<div id='id-section17'/>   
### Entry 17: 2020-02-04, Tuesday.   



------
<div id='id-section18'/>   


### Entry 18: 2020-02-05, Wednesday.   
Need to go through and fix the fastqc file.
##### Objectives
1. Review our progress on read cleaning and visualizing QC
2. Start mapping each set of cleaned reads to a reference genome
*  Made this script in R and transferred to Bash. 
3. Visualize sequence alignment files
4. Process our sam files by..
* converting to binary format and sorting our coordinates
* removing PCR duplicates
* indexing for fast future lookup
5. Calculate mapping stats to assess quality of the result
6. Learn how to put separate bash scripts into a "wrapper" that runs them all
* "The one script to rule them all" 

###### Mapping Cleaned and trimmed reads..
* Reference Genome..
* congenie.org ; Good reference for conifers 
* 12 Haploid chromosomes...24 total.
* What is the N50; how much of the genome is assembled into large contigs(?)
1. - - - -- -- -- --- --- --- **----** ------- -------- --------- ; sum = 668 Mb
2. Take 50% ^ = 334 Mb; start with the biggest contig and move towards smaller until you have reached a sum of 334 Mb. 
3. N50 is the smallest contig at which all of the sum of the larger to smaller contigs = 334 Mb. 
4. N50 = 101,375 bp in this case. 
5. N50 is usually a metric to understand the data you are looking at..
* You usually decide to use this to compare individuals within species.
* Generally speaking, bigger is better, because it gives you better spatial information
##### Code for Today 
1. We don't need to actually download the genome because it is already in a file in the directory...
2. This is a code for us to get the genome.
3. wget; handy tool for us to get info from the internet and download it.
```
cd /data/project_data/RS_ExomeSeq/ReferenceGenomes/Pabies1.0-genome_reduced.fa
```

##### Code for the Wrapper script
```
# We will use this as a wrapper to run our different wrapping scripts
#!/bin/cash # What every script in bash begins with. 

```

##### Mapping
1. Using bwa...
```
bwa mem -t 1 -M ${ref}...
```
2. mem algorithm is fast and accurate for reads
3. -t = how many threads? (CPUs) we are setting 1, but you can up it to your choice on your own
4. -M; setting that labels a read with a special flag ; if when bwa maps it to the genome, it splits that read to more than one contig
* This is possible because our DNA is very fragmented, so our reads could be on two contigs that are close together on the same chromosome.
5. ${ref} = our reference genome
6. ${forward} = our forward read
7. ${reverse} = our  reverse read.

##### Writing loops in bash
for forward in [...] 
*do*
commands
commands
*done*


##### Reminders
When you push data to github, you need to then pull on Bash using the git pull command, in order to pull your data/scripts/etc into Bash
```
screen

bash mypipeline.sh

Cntrl A + cntrl D

screen -r # opens back up your mypipeline.sh

```
------
<div id='id-section19'/>   


### Entry 19: 2020-02-06, Thursday.   



------
<div id='id-section20'/>   


### Entry 20: 2020-02-07, Friday.   



------
<div id='id-section21'/>   


### Entry 21: 2020-02-10, Monday.   



------
<div id='id-section22'/>   


### Entry 22: 2020-02-11, Tuesday.   



------
<div id='id-section23'/>   


### Entry 23: 2020-02-12, Wednesday.   

#### Population Genomics Day 3
##### Class Objectives
1. Review our progress on mapping
2. Calculate mapping statistics to assess quality of the result
3. Visusalize sequence alignment files
4. Introduce use of genotype-likelihoods for analyzing diversity in low coverage sequences
5. Use the 'ANGSD' program to calcullate diversity stats, Fsts, and PCA
* Analysis of next generation sequencing data



##### Code for today
1. Look at a SAM file using 'head' and 'tail'
```
tail -n 100 BRB_04.sam 
```
A sam file is a tab delimited text file that stores information about the alignment of reads in a FASTQ file to reference genome or transcriptome. For each read in a FASTQ file, there's a line in the SAM file that includes;
1. the read, aka query name,
2. a FLAG (number with information about mapping success and orientation and whether the read is the left or right read)
3. the reference sequence name to hwich the read mapped
4. the leftmost position in the reference where the read mapped

2. Flagstat gives us some basic stats on our sam files
```
samtools flagstat BRB_01.sam
```
* 2379308 + 0 in total (QC-passed reads + QC-failed reads) = number of reads
134530 + 0 secondary = *number of reads that weren't duplicated?*
0 + 0 supplementary
0 + 0 duplicates
2241464 + 0 mapped (94.21% : N/A)
2244778 + 0 paired in sequencing
1122389 + 0 read1
1122389 + 0 read2
1380290 + 0 properly paired (61.49% : N/A) ; *how many reads were properly paired together*
2027840 + 0 with itself and mate mapped
79094 + 0 singletons (3.52% : N/A) ; *how many were singletons...mapped with no other mate*
633316 + 0 with mate mapped to a different chr
318077 + 0 with mate mapped to a different chr (mapQ>=5)

3. Write a loop to iterate through your individual files for the .bam files for flagstats
```
for file in ${output}/BWA/${mypop}*sorted.rmdup.bam
  
  do 
    f=${file/.sorted.rmdup.bam/}
    name=`basename ${f}`
    echo ${name} >> ${myrepo}/myresults/${mypop}.names.txt
    samtools flagstat ${file} | awk 'NR>=6&&NR<=12 {print $1}' | column -x
  done >> ${myrepo}/myresults/${mypop}.flagstats.txt
```
4.Calculate depth of coverage from our bam files
```
for file in ${output}/BWA/${mypop}*sorted.rmdup.bam

  do 
    samtools depth ${file} | awk '{sum+=$3} END {print sum/NR}' 
  done >> ${myrepo}/myresults/${mypop}.coverage.txt
```
5. Viewing your sequences
```
samtools tview /data/project_data/RS_ExomeSeq/mapping/BWA/BRB_01.sorted.rmdup.bam /data/project_data/RS_ExomeSeq/ReferenceGenomes/Pabies1.0-genome_reduced.fa

```
![](C:\Users\kwarn\Pictures\Genomics_!.PNG)

Genotype likelihood is the best way to handle these kinds of data.

* Take the most probable.

6. Running ANGSD
1. Create list of bam files for samples you want to analyze
2. Estimate genotype likelihooda and allele frequencies after filtering to minimize noise 
3. Use GL's to:
* a. Estimate the SFS
* b. Estimate nucleotide diversities
* c. estimate fst between all populations, or pairwise between sets of populations
* d. perform a genetic PCA based on the estimation of genetic covariance matrix
4. In myscripts, create ANGSD_mypop.sh
```
REF="/data/project_data/RS_ExomeSeq/ReferenceGenomes/Pabies1.0-genome_reduced.fa"

# Estimating GL's and allele frequencies for all sites with ANGSD

ANGSD -b ${output}/${mypop}_bam.list \
-ref ${REF} -anc ${REF} \
-out ${output}/${mypop}_allsites \
-nThreads 1 \
-remove_bads 1 \
-C 50 \
-baq 1 \
-minMapQ 20 \
-minQ 20 \
-setMinDepth 3 \
-minInd 2 \
-setMinDepthInd 1 \
-setMaxDepthInd 17 \
-skipTriallelic 1 \
-GL 1 \
-doCounts 1 \
-doMajorMinor 1 \
-doMaf 1 \
-doSaf 1 \
-doHWE 1 \
# -SNP_pval 1e-6
```
* if we wanted to just the polymorphic sites we would uncomment out the -SNP_pval call. 

##### Helpful bits of Code
1. Reverse order of a list by the time the file was stamped
```
ll -rt
```
2. look at specific files in a folder. This allows you to look at all BRB sam files.
```
 ll BRB*.sam
```
3. Look at first ten rows
```
head BRB_01.sam
```
* Each row of the head is the name of contig and its length in bp (LN)
4. Look at the bottom ten rows
```
tail BRB_01.sam
```
* GWNJ-0842:368:GW1809211440:2:2224:19167:72983   163     MA_186154       822    40       13M1I41M3S      =       1191    469     AAGGAAANGGNGGNGGAATAGTGATGNGAAGTGTTAAGTATGGTCANGAACGGTACNA      AAFJJJJ#FJ#JJ#JJJJJJJJ<<FF#FFJJAJ-FJFFFFJJJJJJ#JJJFJJJJF#F      NM:i:9  MD:Z:7T2A8A5G0A5G11T0A8 AS:i:19 XS:i:0
1. basically the ID of the read - GWNJ-0842:368:GW1809211440:2:2224:19167:72983
2. 163 = flag ; can be interpretated by going to a flag decoder. The read was paired, mapped in a proper pair, mate reverse strand, second in pair.
3. MA_186154; contig in P.abies that the read mapped to
4. 822 ; left most position in the read
5. 40; mapping quality in phred scale. Very unlikely that the read was mapped to the reference genome incorrectly.

------
<div id='id-section24'/>   


### Entry 24: 2020-02-13, Thursday.   



------
<div id='id-section25'/>   


### Entry 25: 2020-02-14, Friday.   



------
<div id='id-section26'/>   


### Entry 26: 2020-02-17, Monday.   



------
<div id='id-section27'/>   


### Entry 27: 2020-02-18, Tuesday.   



------
<div id='id-section28'/>   


### Entry 28: 2020-02-19, Wednesday.   

#### Population Genomics Day 3:

##### Notes

##### Code for today
1.
```
```
2.
```

```
3.
```

```
4.
```

```
5.
```

```
6.
```

```
7. 
```

```
8. 
```

```
9.
```

```
10.
```

```

------
<div id='id-section29'/>   


### Entry 29: 2020-02-20, Thursday.   



------
<div id='id-section30'/>   


### Entry 30: 2020-02-21, Friday.   



------
<div id='id-section31'/>   


### Entry 31: 2020-02-24, Monday.   



------
<div id='id-section32'/>  


### Entry 32: 2020-02-25, Tuesday.   



------
<div id='id-section33'/>  


### Entry 33: 2020-02-26, Wednesday.   
#### Learning Objectives
1. Review Red Spruce ecology and biogeography and the transcriptomics experimental design
2. Articulate the questions we can address and hypotheses we can test with this experimental design
3. Understand the general work flow or "pipeline" for processing and analyzing RNAseq data
4. Review how to make/write bash script and how to write a script to process files in batches
5. Visualize and interpret Illumina data quality (Run FastQC on raw and cleaned reads).
6. Start mapping reads and quantifying abundance simultaneously using Salmon.
7. Import quant.sf files generated by Salmon into DESeq2 (R package) for analysis and visualization.
8. Add to your growing list of bioinformatics tricks (take notes!). 

#### Notes for Today
1. So far, anything that has been done with this new dataset is that they have been cleaned with trimmomatic

#### Experimental Design
1. Red spruce is a montane coniferous tree, and sometimes in warmer, less ideal habitats. 
2. Sampled genetic variation from sites that were cool/wet and hot/dry...based on historic climate data. *These were grown in common gardens*
** Bio18 = warmest quarter and Bio5= temp of warmest month
3. Test..
4. Design
** 5 familes from each condition
** Experimental treatments:
1. control: Watered every day 16:* Light/dark cycle at 23/17C temp.
2. Heat 16:8 Light dark cycle, oscillating temps from 35-26 - represents 50% increases in temp
3. Heat + Drought,
** Sampled each condition at three time points ( Day 0, 5, 10)..Seedlings were young when experiment started.
** They chose extract RNA from whole seedlings: root, stem, needle tissues...ground them up together so differences among them will be lost. 
1. To do each tissue seperately would triple the cost of the experiments
2. Day 5 had fewer extractions that actually worked. 
5. Total of 76 samples.
6. Only have R1 because this is 3'->5' RNA data: 3' tag sequencing
#### Library Prep and sequencing
1. Benefits of 3' tag sequencing = you only need a little bit of RNA to do this
2. Samples were sent to Cornell for 3' tag sequencing
3. Library prep follows the LexoGen protocol and sequencing was on 1 lane of NExtSeq500 (1x86 bp reads). 
4. Drawbacks: RNA samples are really easy to contaminate and degrade. Challenge is more degradation because of the OH- tag on RNA that DNA doesn't have that makes it much more reactive and labile. 
5. Samples were demiltiplexed and named according to the convention: *POP_FAM_TRT_DAY*
#### What questions can we ask with this experimental design with these data?
1. Factors that we have:
** Treatment: Control, heat, heat/dry
** Source Climate (2: Hot/dry and cool/wet)
** Time (0, 5, 10)
2. Is there a change in expression of genes from individuals from different source climates over time in each of the treatments 
** expression ~ time + source climate + (time x SC) + *family* - might be hard to add because we only have individual samples. 
3. Do individuals coming from different environments have different expressions within the same conditions. 
** expression ~ Source Climate + treatment
** expression ~ SC + trt + (SC + trt)
#### Main Questions of Interest
1. Do families from different source climates differ in gene expression
2. Is there a rtanscriptome wide response to heat stress? Does this change when the additional stress of drought is added?
3. Is there a significant interaction between source climate and srtess treatment, such that families from hot/dry climates have a unique expression repsonse to heat or heat+drought compareison to families from cool/wet cliamtes
4. Which specific genes atre involved in the above responses, and tdo they reveal functional enrichment of particular pathways
5. Do DE genes or pathways show evidence of positive selection
6. Can we use associatoin mapping to identify eQTL associated with DE experiments

#### Data Pipeline
1. FAstQC-> trimmomatic -> FastQC
2. Reference transcriptome:
3. 66,632 unigenes consisting of 26,437 high confidence gene models, 32,150 medium-condfidence gene models, and 8,045 low-confidence gene models.
** transcript isoforms might be included -> Important to consider
** Say you had many transcripts from 1 gene, you can collapse all genes to the gene region and analyze these as 1 gene. 
4. Use Salmon to map reads and quantify abundance
5. Import the data into DESeq2 in R for data normalization, visualization, and statistical tests for differential gene expression.

#### Groups of Population
Jay C and D
#### Code for Today
1. Code to get to our fastq data
```
cd /data/project_data/RS_RNASeq/fastq
ls

```
2. Created fastqc_trans.sh script in bash
```
cp fastqc.sh fastqc_trans.sh
vim fastqc_trans.sh

# Here is the script we wrote for this

# push everything from git
git add -A
git commit -m "comment your changes"
git push

```
3. Can use git head code to back up commits...ran into problems with ANGSD and this helped.
```

```
4. Ran fastqc for the cleaned reads -> wrote a new script for this as well. 
```

```
5.
```

```
6.
```

```


------
<div id='id-section34'/>  


### Entry 34: 2020-02-27, Thursday.   



------
<div id='id-section35'/>   


### Entry 35: 2020-02-28, Friday.   



------
<div id='id-section36'/>   


### Entry 36: 2020-03-02, Monday.   



------
<div id='id-section37'/>   


### Entry 37: 2020-03-03, Tuesday.   



------
<div id='id-section38'/>   


### Entry 38: 2020-03-04, Wednesday.   

#### Class Objectives
1. Should have installed packages in R
2. Map clean reads and quantify abundance simultaneously using Salmon package
3. Assess mapping rate (Salmon log files); explore mapping to different reference transcript sets
4. Generate compiled counts matrix (all 76 samples) from individual quant.sf files using tximport
5. Move the data matrix to your machine
6. Import data matric and sample information into R abd DESeq2
7. Normalize, visualize and analyze exoressuib data using DESeq2

#### Notes for Today

#### Code for Today
1. Write a loop for running Salmon with your populations
```
#!/bin/bash
#index = /data/project_data/RS_RNASeq/ReferenceTranscriptome/Pabies_HC27_index
cd /data/project_data/RS_RNASeq/fastq/cleanreads

for file in JAY_02_C*.cl.fq

do

  salmon quant -i /data/project_data/RS_RNASeq/ReferenceTranscriptome/Pabies_HC27_index -l A -r ${file} --validateMappings -o /data/project_data/RS_RNASeq/salmon/cleanedreads/${file}


done

for file in JAY_02_D*.cl.fq

do

  salmon quant -i /data/project_data/RS_RNASeq/ReferenceTranscriptome/Pabies_HC27_index -l A -r ${file}  --validateMappings -o /data/project_data/RS_RNASeq/salmon/cleanedreads/${file}

done

```
2. Exploring the mapping rate of all of our samples mapped
```
grep -r --include \*.log -e 'Mapping rate'
```
* Here we mapped to high quality transcripts to see if we would have higher quality mapping rates as well...we did not see this. 
* Rates that Melissa had before ~27% : percent of reads in a given file that mapped on the genome with high quality, high confidence index file (reference) that we used. 
* Used our cleaned reads, we got a much much lower mapping rate
* Can it be that we mapped the wrong reference?
* confidence assessments are assigned by blasts through the norway spruce and other plant species...you get a percent identitiy. 
* High confidence genese are blasted with 70%  or higher homology
* Median - 30-70%
* low = >0%
* There are not a lot of conifer specific genes on genbank..so that could be something for us to keep in mind. 
3. Rewriting the loop
```
```
* When making the Salmon Index...
4.
```
salmon index <file you want it to index or reference>
```
* cds is all, rather than the high quality
* we altered the K flag...instead of 31, we changed it to 27 which made our mapping worse. 
* With this data being 3' tag seq are we getting far enough past UTR
5. Look into quant.sf files : Must be in the /salmon/allmapping/<Your Pop File>
```
head -n 100 quant.sf
```
![Quant.sf](https://github.com/Katelynn62/Ecological_Genomics/blob/master/quant.sf_snapshot.PNG) 
* MA - individual transcripts
6
```
```
7.
```
```
8.
```
```
9.
```
```
10.
```
```



------
<div id='id-section39'/>   


### Entry 39: 2020-03-05, Thursday.   



------
<div id='id-section40'/>   


### Entry 40: 2020-03-06, Friday.   



------
<div id='id-section41'/>  


### Entry 41: 2020-03-09, Monday.   



------
<div id='id-section42'/>   


### Entry 42: 2020-03-10, Tuesday.   



------
<div id='id-section43'/>   


### Entry 43: 2020-03-11, Wednesday.   

** Spring Break

------
<div id='id-section44'/>   


### Entry 44: 2020-03-12, Thursday.   



------
<div id='id-section45'/>   


### Entry 45: 2020-03-13, Friday.   



------
<div id='id-section46'/>   


### Entry 46: 2020-03-16, Monday.   

** Class Canceled

------
<div id='id-section47'/>   


### Entry 47: 2020-03-17, Tuesday.   


------
<div id='id-section48'/>   


### Entry 48: 2020-03-18, Wednesday.   

#### Learning Objectives
1. Recap on Mapping 3' RNA-seq data to transcriptome and assembly of data matrix
2. Import data matrix and sample information into R ansd DESeq2
3. Normalize, visualize and analyze expression data using DESeq2

#### Notes for Today

* Troubleshooting and improving mapping rate
1. When writing for loops for mapping reads to reference transcriptome, reads were extremely low. ~2%
2. Many of our reads were not mapping becayse the reference we had selected included only the coding region
3. When working with 3' RNAseq data, much of the data is likely to be in the untranslated region, so this region + coding region was combined to create a reference transcriptome with mapping rate of 52%

* Assembling a counts matrix using 'tximport' in R
```
library(tximportData)
library(tximport)

#locate the directory containing the files. 
dir <- "/data/project_data/RS_RNASeq/salmon/"
list.files(dir)

# read in table with sample ids
samples <- read.table("/data/project_data/RS_RNASeq/salmon/RS_samples.txt", header=TRUE)

# now point to quant files
all_files <- file.path(dir, samples$sample, "quant.sf")
names(all_files) <- samples$sample

# what would be used if linked transcripts to genes
#txi <- tximport(files, type = "salmon", tx2gene = tx2gene)
# to be able to run without tx2gene
txi <- tximport(all_files, type = "salmon", txOut=TRUE)  
names(txi)

head(txi$counts)

countsMatrix <- txi$counts
dim(countsMatrix)
#[1] 66069    76

# To write out
write.table(countsMatrix, file = "RS_countsMatrix.txt", col.names = T, row.names = T, quote = F)
```

* Import counts matrix into R and run DESeq

```
## Set your working directory
setwd("YOURWORKINGDIRECTORY")

## Import the libraries that we're likely to need in this session
library(DESeq2)
library(dplyr)
library(tidyr)
library(ggplot2)
library(scales)
library(ggpubr)
library(wesanderson)
library(vsn)  ### First: BiocManager::install("vsn") AND BiocManager::install("hexbin")

## Import the counts matrix
countsTable <- read.table("RS_cds2kb_countsMatrix.txt", header=TRUE, row.names=1)
head(countsTable)
dim(countsTable)
countsTableRound <- round(countsTable) # Need to round because DESeq wants only integers
head(countsTableRound)

## Import the samples description table - links each sample to factors of the experimental design.
# Need the colClasses otherwise imports "day" as numeric which DESeq doesn't like, coula altneratively change to d0, d5, d10
conds <- read.delim("RS_samples.txt", header=TRUE, stringsAsFactors = TRUE, row.names=1, colClasses=c('factor', 'factor', 'factor', 'factor'))
head(conds)
dim(conds)


## Let's see how many reads we have from each sample:
colSums(countsTableRound)
mean(colSums(countsTableRound))
barplot(colSums(countsTableRound), las=3, cex.names=0.5,names.arg = substring(colnames(countsTableRound),1,13))
abline(h=mean(colSums(countsTableRound)), col="blue", lwd =2)

#What is the average number of counts per gene
rowSums(countsTableRound)
mean(rowSums(countsTableRound)) #mean is being driven up by very very large genes
median(rowSums(countsTableRound)) # gene expression is NOT a normal distribution.
### Shows dispersion across genes; differences in magnitude of expression

#What is the average number of counts per gene per sample
apply(countsTableRound, 2, mean)

## Create a DESeq object and define the experimental design here with the tilde

ddsPop = DESeqDataSetFromMatrix(countData = countsTableRound, colData = conds, design = ~ pop + day + treatment)
dim(dds)

# Filter out genes with few reads

ddsPop = ddsPop[rowSums(counts(ddsPop)) > 76]
dim(dds)

## Run the DESeq model to test for differential gene expression: 1) estimate size factors (per sample), 2) estimate dispersion (per gene), 3) run negative binomial glm
ddsPop = DESeq(ddsPop)


# List the results you've generated
resultsNames(ddsPop)
# [1] "Intercept"            "pop_BRU_05_vs_ASC_06"
# [3] "pop_CAM_02_vs_ASC_06" "pop_ESC_01_vs_ASC_06"
# [5] "pop_JAY_02_vs_ASC_06" "pop_KAN_04_vs_ASC_06"
# [7] "pop_LOL_02_vs_ASC_06" "pop_MMF_13_vs_ASC_06"
# [9] "pop_NOR_02_vs_ASC_06" "pop_XBM_07_vs_ASC_06"
# [11] "day_10_vs_0"          "day_5_vs_0"          
# [13] "treatment_D_vs_C"     "treatment_H_vs_C" 
```
* Ways to explore data analysis
1. explore the various results for a given experimental design: ``` resultsNames(dds) ```
2. Det up your DESeq object with different experimental designs. ``` design= ~ climate + treatment + climate:treatment ```
3. Subset your data to exclude/include different factors, run different designs; ``` select``` and ``` subset ``` are handing functions in R for subsetting your ```countsMatrix``` or ```conds``` table. *we will use this for creating Day 10 samples.*




------
<div id='id-section49'/>   


### Entry 49: 2020-03-19, Thursday.   



------
<div id='id-section50'/>   


### Entry 50: 2020-03-20, Friday.   



------
<div id='id-section51'/>   


### Entry 51: 2020-03-23, Monday.   



------
<div id='id-section52'/>   


### Entry 52: 2020-03-24, Tuesday.   



------
<div id='id-section53'/>   


### Entry 53: 2020-03-25, Wednesday.   



------
<div id='id-section54'/>   


### Entry 54: 2020-03-26, Thursday.   



------
<div id='id-section55'/>   


### Entry 55: 2020-03-27, Friday.   



------
<div id='id-section56'/>   


### Entry 56: 2020-03-30, Monday.   



------
<div id='id-section57'/>   


### Entry 57: 2020-03-31, Tuesday.   



------
<div id='id-section58'/>   


### Entry 58: 2020-04-01, Wednesday.   



------
<div id='id-section59'/>  


### Entry 59: 2020-04-02, Thursday.   



------
<div id='id-section60'/>   
### Entry 60: 2020-04-03, Friday.   



------
<div id='id-section61'/>   
### Entry 61: 2020-04-06, Monday.   



------
<div id='id-section62'/>   
### Entry 62: 2020-04-07, Tuesday.   



------
<div id='id-section63'/>   
### Entry 63: 2020-04-08, Wednesday.   



------
<div id='id-section64'/>   
### Entry 64: 2020-04-09, Thursday.   



------
<div id='id-section65'/>   
### Entry 65: 2020-04-10, Friday.   



------
<div id='id-section66'/>   
### Entry 66: 2020-04-13, Monday.   



------
<div id='id-section67'/>   
### Entry 67: 2020-04-14, Tuesday.   



------
<div id='id-section68'/>   
### Entry 68: 2020-04-15, Wednesday.   



------
<div id='id-section69'/>   
### Entry 69: 2020-04-16, Thursday.   



------
<div id='id-section70'/>   
### Entry 70: 2020-04-17, Friday.   



------
<div id='id-section71'/>   
### Entry 71: 2020-04-20, Monday.   



------
<div id='id-section72'/>   
### Entry 72: 2020-04-21, Tuesday.   



------
<div id='id-section73'/>   
### Entry 73: 2020-04-22, Wednesday.   



------
<div id='id-section74'/>   
### Entry 74: 2020-04-23, Thursday.   



------
<div id='id-section75'/>   
### Entry 75: 2020-04-24, Friday.   



------
<div id='id-section76'/>   
### Entry 76: 2020-04-27, Monday.   



------
<div id='id-section77'/>   
### Entry 77: 2020-04-28, Tuesday.   



------
<div id='id-section78'/>   
### Entry 78: 2020-04-29, Wednesday.   



------
<div id='id-section79'/>   
### Entry 79: 2020-04-30, Thursday.   



------
<div id='id-section80'/>   
### Entry 80: 2020-05-01, Friday.   



------
<div id='id-section81'/>   
### Entry 81: 2020-05-04, Monday.   



------
<div id='id-section82'/>   
### Entry 82: 2020-05-05, Tuesday.   



------
<div id='id-section83'/>   
### Entry 83: 2020-05-06, Wednesday.   



------
<div id='id-section84'/>   
### Entry 84: 2020-05-07, Thursday.   



------
<div id='id-section85'/>   
### Entry 85: 2020-05-08, Friday.   
